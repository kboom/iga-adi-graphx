# Running locally

To run this project locally make sure that you have "include dependencies from provided scope" checked. This project depends on spark which is shipped with the container so it does not need to be included twice.

# Running in a real cluster

docker pull mesosphere/spark:2.5.0-2.2.1-hadoop-2.7


https://stackoverflow.com/questions/37132559/add-jars-to-a-spark-job-spark-submit
